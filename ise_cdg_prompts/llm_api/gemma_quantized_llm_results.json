{"inputs": [], "outputs": ["<bos><start_of_turn>user\nStart Markdown 1: # Exercises\nYou could write the function `get_mae` yourself. For now, we'll supply it. This is the same function you read about in the previous lesson. Just run the cell below.\nStart Code 1: def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\nStart Markdown 2: Keras NN model can be evaluated on lots of metrics with just passing the metric name, but it is not the case for ROC_AUC score, so we will define our own auc metric function:\nStart Code 2: def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\nStart Markdown 3: # Wordcloud\nStart Code 3: #Custom function to extract text from variety column\ndef get_text(column):\n    words = ''\n    for text in column:\n        words += text\n    return words\nStart Markdown 4: Therefore, we'll create this little function to just return the single number we need given a pair of variables:\nStart Code 4: def corr(x,y): return np.corrcoef(x,y)[0][1]\n\ncorr(housing.MedInc, housing.MedHouseVal)\n\nGenerate markdown for the bottom code according to the four samples above\n Code: def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)\n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()<end_of_turn>\n<start_of_turn>model\n## Markdown for the bottom code:\n\n```python\ndef tr_plot(tr_data, start_epoch):\n    # Plot the training and validation data\n    tacc = tr_data.history['accuracy']\n    tloss = tr_data.history['loss']\n    vacc = tr_data.history['val_accuracy']\n    vloss = tr_data.history['val_loss']\n    Epoch_count = len(tacc) + start_epoch\n    Epochs = []\n    for i in range(start_epoch, Epoch_count):\n        Epochs.append(i + 1)\n    index_loss = np.argmin(vloss)  # this is the epoch with the lowest validation loss\n    val_lowest = vloss[index_loss]\n    index_acc = np.argmax(vacc)\n    acc_highest = acc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label = 'best epoch='+ str(index_loss + 1 + start_epoch)\n    vc_label = 'best epoch='+ str(index_acc + 1 + start_epoch)\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n    axes[0].plot(Epochs, tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs, vloss, 'g', label='Validation loss')\n    axes[0].scatter(index_loss + 1 + start_epoch, val_lowest, s=150, c='blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot(Epochs, tacc, 'r', label='Training Accuracy')\n    axes[1].plot(Epochs, vacc, 'g', label='Validation Accuracy')\n    axes[1].scatter(index_acc + 1 + start_epoch, acc_highest, s=150, c='blue', label=vc_label)\n    axes[1].set_title('Training", "<bos><start_of_turn>user\nStart Markdown 1: # \ud83d\ude84 Training Function\nStart Code 1: def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    return epoch_loss\nStart Markdown 2: ##Separate Lungs\nStart Code 2: def seperate_lungs(image):\n    #Creation of the markers as shown above:\n    marker_internal, marker_external, marker_watershed = generate_markers(image)\n    \n    #Creation of the Sobel-Gradient\n    sobel_filtered_dx = ndimage.sobel(image, 1)\n    sobel_filtered_dy = ndimage.sobel(image, 0)\n    sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n    sobel_gradient *= 255.0 / np.max(sobel_gradient)\n    \n    #Watershed algorithm\n    watershed = morphology.watershed(sobel_gradient, marker_watershed)\n    \n    #Reducing the image created by the Watershed algorithm to its outline\n    outline = ndimage.morphological_gradient(watershed, size=(3,3))\n    outline = outline.astype(bool)\n    \n    #Performing Black-Tophat Morphology for reinclusion\n    #Creation of the disk-kernel and increasing its size a bit\n    blackhat_struct = [[0, 0, 1, 1, 1, 0, 0],\n                       [0, 1, 1, 1, 1, 1, 0],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [0, 1, 1, 1, 1, 1, 0],\n                       [0, 0, 1, 1, 1, 0, 0]]\n    blackhat_struct = ndimage.iterate_structure(blackhat_struct, 8)\n    #Perform the Black-Hat\n    outline += ndimage.black_tophat(outline, structure=blackhat_struct)\n    \n    #Use the internal marker and the Outline that was just created to generate the lungfilter\n    lungfilter = np.bitwise_or(marker_internal, outline)\n    #Close holes in the lungfilter\n    #fill_holes is not used here, since in some slices the heart would be reincluded by accident\n    lungfilter = ndimage.morphology.binary_closing(lungfilter, structure=np.ones((5,5)), iterations=3)\n    \n    #Apply the lungfilter (note the filtered areas being assigned -2000 HU)\n    segmented = np.where(lungfilter == 1, image, -2000*np.ones((512, 512)))\n    \n    return segmented, lungfilter, outline, watershed, sobel_gradient, marker_internal, marker_external, marker_watershed\nStart Markdown 3: ### Data Cleaning\n\n* Adjust the `Crash Severity ID` since it does not line up in a same step scale as the `Crash Severity`\n    * There are values like `201905.0` and `201904.0` which should just be 5 & 4 respectively.\n    * This lines up with the ID captured in column `CRASHSEVERITY`\n    \n    \n* 7 columns have issues with small variations in category names, for instance 'Other' is encoded as '9 - Other' and '99 - Other', or \"M - Male\" and \"Male\".  I will treat those as the same values;\n    * Columns with this issue present:\n        * `GENDER`\n        * `CRASHSEVERITY`\n        * `INJURIES`\n        * `LIGHTCONDITIONSPRIMARY`\n        * `ROADCONDITIONSPRIMARY`\n        * `ROADSURFACE`\n        * `WEATHER`\n        \n        \n* Change the column type of `LOCALREPORTNO` to 'object'\nStart Code 3: # Fix the Crash Severity ID\ndef CrashSeverityIDFixed(x):\n    if x == 201905.0:\n        return 5\n    elif x == 201904.0:\n        return 4\n    elif x == 201903.0:\n        return 3\n    elif x == 201902.0:\n        return 2\n    elif x == 201901.0:\n        return 1\n    else: \n        return x\n    \ndf['CRASHSEVERITYID'] = df['CRASHSEVERITYID'].apply(lambda x: CrashSeverityIDFixed(x))\n\n# Fix Gender\ndf.loc[(df['GENDER'] == 'MALE'), 'GENDER']='M - MALE'\ndf.loc[(df['GENDER'] == 'FEMALE'), 'GENDER']='F - FEMALE'\n\n# Fix CRASHSEVERITY\ndf.loc[(df['CRASHSEVERITY'] == '1 - FATAL'), 'CRASHSEVERITY']='1 - FATAL INJURY'\ndf.loc[(df['CRASHSEVERITY'] == '3 - PROPERTY DAMAGE ONLY (PDO)'), 'CRASHSEVERITY']='5 - PROPERTY DAMAGE ONLY'\n\n# Create a simple loop to fix the other 6 columns\ncols = ['INJURIES', 'LIGHTCONDITIONSPRIMARY', 'ROADCONDITIONSPRIMARY', 'ROADSURFACE', 'WEATHER', 'CRASHSEVERITY']\n\nfor col in cols:\n    df[col] = df[col].str.split(\" - \", 1).str[1]\n    \n# Adjust some column types\ndf = df.astype({\"CRASHSEVERITYID\": object, \"LOCALREPORTNO\": object})\nStart Markdown 4: To compare different approaches to dealing with missing values, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model.\nStart Code 4: from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\nGenerate markdown for the bottom code according to the four samples above\n Code: (In the previous step we also replaced the categorical variables with their integer codes, since some of the models we'll be building in a moment require that.)\n\nNow we can create our independent variables (the `x` variables) and dependent (the `y` variable):<end_of_turn>\n<start_of_turn>model\n## Summary of the Code\n\nThis code encompasses several functions and tasks:\n\n**1. Train One Epoch:**\n- Defines a function `train_one_epoch` that trains a model for one epoch.\n- It involves scaling the loss, backward pass, and zeroing gradients.\n- It also calculates the epoch loss and updates various metrics.\n\n**2. Separate Lungs:**\n- Defines a function `separate_lungs` that segments lungs from an image.\n- It involves creating markers, applying Sobel filters, performing watershed algorithm, and applying black-tophat morphology.\n- It finally generates a lung filter and applies it to the image.\n\n**3. Data Cleaning:**\n- Fixes the `Crash Severity ID` and other inconsistencies in the data.\n- It adjusts column types and categories for various columns.\n\n**4. Score Dataset:**\n- Defines a function `score_dataset` that calculates the mean absolute error (MAE) of a model.\n- It uses this function to compare different approaches to handling missing values.\n\n**Additional Notes:**\n\n- The code references several functions and variables from other parts of the program, therefore it might not be complete on its own.\n- The code uses the `tqdm` library to display a progress bar during training.\n- The code uses the `amp` library for mixed precision training.\n- The code utilizes various libraries such as `ndimage`, `morphology`, and `sklearn.ensemble` for image processing and model training.\n\n**Overall, this code demonstrates various techniques for training and preprocessing models, as well as handling missing values and performing data cleaning.**"]}