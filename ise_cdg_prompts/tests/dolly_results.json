[["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef initialize_parameters(layer_dims):\n    parameters = {}\n    L = len(layer_dims)           \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n    return parameters  \n#Summary: Initialize the Weights and Biases for all layers\n#Code\ndef Grade(mark):\n  gr = \"\"\n  if mark<50 and mark>=35:\n    gr = 'Failed'\n  elif mark >= 50 and mark<60:\n    gr = \"Pass\"\n  elif mark >=60 and mark<70:\n    gr = \"Satisfactory\"\n  elif mark >=70 and mark<80:\n    gr = \"Good\"\n  elif mark >=80 and mark<90:\n    gr = \"very good\"\n  elif mark >=90 and mark<=100:\n    gr = \"Excellent\"\n  else:\n    gr = \"Invalid\"\n  return gr\n\nGrade(120)\n#Summary: Write a function that takes the GPA of a students and returns the Grade (fail, sat, good, very good, or excellent)\u200b\n#Code\ndef nothing_but_the_best(train):\n    # Nothing but the best: A person that paid an unusually high price (z-score) for his ticket.\n    train['nothing_but_the_best'] = np.where((train['Fare'] > train['Fare'].mean() + 3 * train['Fare'].std()), 1, 0)\n    return train['nothing_but_the_best']\n\ntrain['nothing_but_the_best'] = nothing_but_the_best(train)\n#Summary: Nothing But The Best\nA person that paid an unusually high price (z-score) for his ticket.\n#Code\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n#Summary: Define the loss function\n#Code\ndef binning_ticketnumbers_column(dataframe_train, dataframe_validation, ticketnumbers_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'quantile')\n\n    dataframe_train[ticketnumbers_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketnumbers_column].values.reshape(-1,1))\n    dataframe_validation[ticketnumbers_column] = k_bins_discretizer.transform(dataframe_validation[ticketnumbers_column].values.reshape(-1,1))\n\n    dataframe_train[ticketnumbers_column] = dataframe_train[ticketnumbers_column].astype('int64')\n    dataframe_validation[ticketnumbers_column] = dataframe_validation[ticketnumbers_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketnumbers_column(train_titanic, validation_titanic, 'TicketNumbers')\n#Summary: Transforming the \"TicketNumbers\" column in categorical Bins.\n\n#Code\ndef pimp_my_ride(train):\n    # Pimp my ride: Third class AND teenager AND Alone AND Spent more than 5 dollars\n    train['pimp_my_ride'] = np.where((train['Pclass'] == 3) &\n                                     (train['Age'] < 16) &\n                                     (train['SibSp'] < 1) &\n                                     (train['Parch'] < 1) &\n                                     (train['Fare'] > 5),\n                                     1, 0)\n    return train['pimp_my_ride']\n\ntrain['pimp_my_ride'] = pimp_my_ride(train)\n#Summary:", "Pimp My Ride\nThird class AND teenager AND Alone AND Spent more than 5 dollars"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\n\ndef AddList(x, y):\n  z = []\n  for i in range(0, len(x)):\n    r = x[i]+y[i]\n    z.append(r)\n  return z\n\nA = [1, 2, 3, 8]\nB = [3, 4, 5, 2]\nC = AddList(A, B)\nC\n#Summary: write a function that receives two lists and return a list that contains the addition of the corresponding element\n#Code\ndef compute_cost(AL, Y,parameters,lambd ):\n    m = Y.shape[1]\n    cost = (-1/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))\n    cost = np.squeeze(cost)  \n    \n    L = len(parameters) // 2 \n    regularization = 0;\n    \n    for l in range(L):\n        regularization +=  np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n        \n    L2_regularization_cost = lambd / (2 * m) * regularization\n    cost = cost + L2_regularization_cost\n    return cost\n#Summary: Calculate the cross-entropy cost function\n#Code\ntqdm.pandas()\ndef ctc_min_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  common_token=set(que1).intersection(set(que2))\n  return len(common_token)/(min(len(que1),len(que2))+0.0001)\ndf['ctc_min']=df.progress_apply(ctc_min_cal,axis=1)\n#Summary: \nctcmin=common token count/min(no of tokens in q1,no of tokens in q2)\n\n#Code\ndef imputing_embarked(dataframe, embarked_column):\n\n    dataframe[embarked_column] = dataframe[embarked_column].fillna('cherbourg')\n\n    return dataframe\n\ntrain_titanic = imputing_embarked(train_titanic, 'Embarked')\n#Summary: Imputing the \"Embarked\" column in the training data using the values from similar passengers to the missing ones.\n#Code\ndef SumMul(x, y, z):\n    s = x+y+z\n    m = x*y*z\n    return s, m\n\nrs, rm = SumMul(23, 1, 2)\nprint (\"The sum is \", rs , \" and the multiplication is \", rm)\n#Summary: Write a function that takes three numbers and returns the sum and multiplaction of the numbers\n\n#Code\ndef wait_a_sec_hold_on(train):\n    # Wait a sec. hold on!: Young male alone with nannies everywhere (or at least one nanny).\n    train['wait_a_sec_hold_on'] = np.where((train['Age'] < 30) &\n                                           (train['Sex'] == 'male') &\n                                           (train['SibSp'] == 0) &\n                                           (train['Parch'] == 1),\n                                           1, 0)\n    return train['wait_a_sec_hold_on']\n\ntrain['wait_a_sec_hold_on'] = wait_a_sec_hold_on(train)\n#Summary:", "Wait a sec. hold on!\nYoung male alone with nannies."], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef calculate_max_min_difference(df, m):\n\n    # Calculate max-min difference by window\n    for fs in features_list:\n        df[fs + '_' + str(m) + '_max_min'] = df[fs + '_' + str(m) + '_max'] - df[fs + '_' + str(m) + '_min']\n        \n    return df\n#Summary: max-min difference\nDifference between maximum and minimum values\n#Code\ndef binning_ticketletters_column(dataframe_train, dataframe_validation, ticketletters_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[ticketletters_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketletters_column].values.reshape(-1,1))\n    dataframe_validation[ticketletters_column] = k_bins_discretizer.transform(dataframe_validation[ticketletters_column].values.reshape(-1,1))\n\n    dataframe_train[ticketletters_column] = dataframe_train[ticketletters_column].astype('int64')\n    dataframe_validation[ticketletters_column] = dataframe_validation[ticketletters_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketletters_column(train_titanic, validation_titanic, 'TicketLetters')\n#Summary: Transforming the \"TicketLetters\" column in categorical Bins.\n#Code\na = FloatSlider(min=10, max=50, step=1, value=40, description='a', layout=layout)\nb = FloatSlider(min=-5, max=5, step=0.1, value=3, description='b', layout=layout)\nc = FloatSlider(min=10, max=50, step=1, value=26, description='c', layout=layout)\n\ndef fun(a, b, c):\n    h = 0.01\n    t = 10000\n\n    x = np.empty(t+1)\n    y = np.empty(t+1)\n    z = np.empty(t+1)\n\n    x[0]=-0.1\n    y[0]=0.5\n    z[0]=-0.6\n\n    for i in range(t):\n        x[i+1] = x[i] + h*(a*(y[i] - x[i]))\n        y[i+1] = y[i] + h*((c - a)*x[i] - x[i]*z[i] + c*y[i])\n        z[i+1] = z[i] + h*(x[i]*y[i] - b*z[i])\n\n    plot3D(x,y,z)\n#     scatter3D(x,y,z)\n    \ninteract(fun, a=a, b=b, c=c);\n#Summary: Chen attractor\n#Code\ndef addfreq(row):\n  return (row['freq_qid1']+row['freq_qid2'])\ndf['add_freq']=df.apply(addfreq,axis=1)\n\n#Summary: \ntotal frequency of question 1 and question 2 add_freq=freq_qid1+freq_qid2\n\n#Code\ndef build_single_shot_model(n_features: int):\n  model = Sequential(\n    name='Single-Shot-Deep-Learning-Model',\n    layers=[\n      Dense(\n        name='Input-Layer',\n        units=1,\n        activation='relu',\n        input_dim=n_features,\n      ),\n      Dense(\n        name='Output-Layer',\n        units=1,\n        # units=2,\n        activation='sigmoid',\n        # activation='softmax',\n      ),\n    ]\n  )\n\n  model.compile(\n    optimizer=tf.optimizers.Adam(\n      learning_rate=0.00001,\n    ),\n    loss='binary_crossentropy',\n    metrics=['accuracy'],\n  )\n\n  model.summary()\n\n  return model\n#Summary: Architecture\n\n#Code\ndef logistic(r, x):\n    return r * x * (1 - x)\n\nn = 10000\nr = np.linspace(2.5, 4.0, n)\niterations = 1000\nlast = 100\n\nx = 1e-5 * np.ones(n)\nlyapunov = np.zeros(n)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\nfor i in range(iterations):\n    x = logistic(r, x)\n\n    lyapunov += np.log(abs(r - 2 * r * x))\n    \n    if i >= (iterations - last):\n        ax1.plot(r, x, ',k', alpha=.25)\n#         ax1.scatter(r, x, c='k')\n        \nax1.set_xlim(2.5, 4)\nax1.set_title(\"Bifurcation diagram\")\n\nax2.axhline(0, color='k', lw=.5, alpha=.5)\nax2.plot(r[lyapunov < 0], lyapunov[lyapunov < 0] / iterations, '.k', alpha=.5, ms=.5)\nax2.plot(r[lyapunov >= 0], lyapunov[lyapunov >= 0] / iterations,'.r', alpha=.5, ms=.5)\nax2.set_xlim(2.5, 4)\nax2.set_ylim(-2, 1)\nax2.set_title(\"Lyapunov exponent\")\n\nplt.tight_layout()\n#Summary:", "Logistic map"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef indices_with_elem(my_list):\n    if not my_list:\n        print(\"Empty\")\n    else:\n        for i,elem in enumerate(my_list):\n            print(i,elem,end=\"\\n\")\n\nindices_with_elem([1,2,3,4,5])\n#Summary: Write a Python program that prints the elements of a list followed their corresponding indices.\n\nEach element and its index must be on the same line separated by a space.\n\nIf the list is empty, print \"Empty List\".\n#Code\ndef initialize_adam(parameters):\n    L = len(parameters) // 2 \n    v = {}\n    s = {}\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n        s[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        s[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n    return v, s\n#Summary: Initialize adam optimizer parameters that is needed in adam gradients updating\n#Code\ndef calculate_vif(column_list, df):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = column_list\n    vif[\"VIF\"] = [variance_inflation_factor(df.to_pandas().values, i) for i in range(len(column_list))]\n    display(vif)\n    print('\\n')\n    \n    return vif\n#Summary: Perform final VIF check\n#Code\ndef train_validation_splitting(dataframe, identifier_column, target_values):\n\n    train_dataframe = dataframe[dataframe[identifier_column] == 'train']\n    train_dataframe = train_dataframe.drop(columns = [identifier_column])\n    train_dataframe['Survived'] = target_values\n\n    validation_dataframe = dataframe[dataframe[identifier_column] == 'validation']\n    validation_dataframe = validation_dataframe.drop(columns = [identifier_column])\n\n    return train_dataframe, validation_dataframe\n\ntrain_titanic, validation_titanic = train_validation_splitting(titanic_dataframe, 'DataClass', y_variable)\n#Summary: Splitting the merged titanic dataframe in the train and validation dataframes.\n#Code\ndef col_to_one_hot_encoding(dataset, col_name):\n  columns = [ col for col in dataset.columns ]\n  if col_name not in columns: return False\n\n  df = dataset.copy()\n  df_encoded = pd.get_dummies(df[col_name], prefix=col_name, dummy_na=True)\n  df = df.drop(columns=[col_name])\n  df = df.join(df_encoded)\n\n  return df\n#Summary: Spreads all the values of a column into n columns\n\n#Code\ndef business_as_usual(train):\n    # Business as usual: Men with less than 10 dollars who also weren't travelling with their family.\n    train['business_as_usual'] = np.where((train['Sex'] == 'male') &\n                                          (train['SibSp'] + train ['Parch']) < 1 &\n                                          (train['Fare'] < 10),\n                                          1, 0)\n    return train['business_as_usual']\n\ntrain['business_as_usual'] = business_as_usual(train)\n#Summary:", "Business as usual\nMen paid less than 10 dollars who also weren't travelling with their family."], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef fixFeature36(dataset):\n    counts = train[feature].value_counts()\n    dataset[feature] = dataset[feature].cat.add_categories('others').fillna('others')\n    \n    sub30k = counts[counts < 30000].index\n    dataset[feature] = dataset[feature].apply(lambda x: 'others' if x in sub30k else x)\n    \n    mapping = {\n                feature: \n                   {\"na\": \"others\",\n                   \"fixers\": [{\"others\": sub30k}]\n                   }\n              }\n\n    return mapping, dataset\n    \nmapping, train = fixFeature36(train.copy(deep=True))\nprint(f\"Feature: {feature} has {train[feature].isna().sum()} null values and {len(train[feature].unique())} unique categories\")\nprint(train[feature].value_counts())\n\ncategoricalFixerMappings.update(mapping)\n#Summary: Group the categories with too few counts into 'others'\n#Code\na = FloatSlider(min=-10, max=10, step=0.1, value=-5.5, description='a', layout=layout)\nb = FloatSlider(min=-10, max=10, step=0.1, value=3.5, description='b', layout=layout)\nd = FloatSlider(min=-10, max=10, step=0.1, value=-1, description='d', layout=layout)\n\n\ndef fun(a, b, d):\n    h = 0.01\n    t = 100000\n\n    x = np.empty(t+1)\n    y = np.empty(t+1)\n    z = np.empty(t+1)\n\n    x[0]=1\n    y[0]=1\n    z[0]=1\n\n    for i in range(t):      \n        x[i+1] = x[i] + h*(y[i])\n        y[i+1] = y[i] + h*(z[i])\n        z[i+1] = z[i] + h*(-a*x[i] - b*y[i] - z[i] + d*x[i]**3)\n        \n\n    plot3D(x,y,z)\n#     scatter3D(x,y,z)\n\ninteract(fun, a=a, b=b, d=d);\n#Summary: Arneodo attractor\n#Code\ntqdm.pandas()\ndef fir_word_eq_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  if(len(que1)==0 or len(que2)==0):\n    return 0.0\n  if(que1[0]==que2[0]):\n    return 1.0;\n  else:\n    return 0.0;\ndf['fir_word_eq']=df.progress_apply(fir_word_eq_cal,axis=1)\n#Summary: \nfirst_word_equal= 1 if first word of both questions are equal else 0\n\n#Code\ndef plot_model_explainability(\n  X_explainability: pd.DataFrame,\n  coeficients,\n  title: str = ''\n) -> None:\n  df_logistic = pd.DataFrame(list(zip(X_explainability.columns, coeficients[0])))\n  df_logistic = df_logistic.reindex(\n    df_logistic[1].abs().sort_values().index\n  ).set_index(0)\n  ax = df_logistic.plot.barh(\n    width=.6,\n    legend=\"\",\n    figsize = (12, 9)\n  )\n  ax.set_title(\n    title,\n    y = 1.03,\n    fontsize = 16.\n  )\n  _ = ax.set(frame_on = False, xlabel = \"\", xticklabels = \"\", ylabel = \"\")\n\n  for i, label in enumerate(list(df_logistic.index)):\n      score = df_logistic.loc[label][1]\n      ax.annotate('%.2f' % score, (score + (-.12 if score < 0 else .02), i - .2), fontsize = 10.5)\n#Summary: Explainability\nSome models can give you their weights, which can help, to a certain degree, help us understand how they're making decisions\n\n#Code\na = FloatSlider(min=-3, max=3, step=0.01, value=2.01, description='a', layout=layout)\nb = FloatSlider(min=-3, max=3, step=0.01, value=-2.53, description='b', layout=layout)\nc = FloatSlider(min=-3, max=3, step=0.01, value=1.61, description='c', layout=layout)\nd = FloatSlider(min=-3, max=3, step=0.01, value=-0.33, description='d', layout=layout)\n\ndef f(a, b, c, d):\n    t = 100000\n\n    x = np.empty(t+1)\n    y = np.empty(t+1)\n\n    x[0]=0\n    y[0]=0\n\n    for i in range(t):\n        x[i+1] = np.sin(a*y[i]) + c*np.cos(a*x[i])\n        y[i+1] = np.sin(b*x[i]) + d*np.cos(b*y[i])\n\n\n    fig, ax = plt.subplots(figsize=(7, 7))\n    ax.scatter(x, y, c='b', s=0.01)\n    ax.axis('off');\n    \ninteract(f, a=a, b=b, c=c, d=d);\n#Summary:", "Pickover attractor"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef asterick_pattern(n):\n    if n==1:\n        print(\"*\")\n    else:\n        print(\"*\" * n)\n        asterick_pattern(n-1)\n\nasterick_pattern(6)\n#Summary: Write a Python program that prints the pattern of asterisks shown below for a given value of n.\n\nThe program must include a recursive function.\n\nn represents the number of rows in the resulting pattern and the number of asterisks printed on the first row.\n#Code\ntqdm.pandas()\ndef fuzz_sort_ratio_cal(row):\n  return fuzz.token_sort_ratio(row['question1'],row['question2'])\ndf['fuzz_sort_ratio']=df.progress_apply(fuzz_sort_ratio_cal,axis=1)\n\n#Summary: \nfuzz_sort_ratio\n\n#Code\ndef renaming__embarked_values(dataframe, embarked_column):\n\n    dataframe[embarked_column] = dataframe[embarked_column].replace({'S': 'southampton', 'C': 'cherbourg', 'Q': 'queenstown'})\n\n    return dataframe\n\ntrain_titanic = renaming__embarked_values(train_titanic, 'Embarked')\nvalidation_titanic = renaming__embarked_values(validation_titanic, 'Embarked')\n#Summary: Renaming the values of the \"Embarked\" column for the training and validation data.\n#Code\ndef fixFeature15(dataset):\n    counts = train[feature].value_counts()\n    dataset[feature] = dataset[feature].cat.add_categories('sub_3k').fillna('sub_3k')\n    \n    sub3k = counts[counts < 3000].index\n    dataset[feature] = dataset[feature].apply(lambda x: 'sub_3k' if x in sub3k else x)\n    \n    range8k = counts[(counts >= 3000) & (counts <= 8000)].index\n    dataset[feature] = dataset[feature].apply(lambda x: 'range_3k_8k' if x in range8k else x)\n    \n    mapping = {\n                feature: \n                   {\"na\": \"sub_3k\",\n                   \"fixers\": [{\"sub3k\": sub3k}, {\"range_3k_8k\": range8k}]\n                   }\n              }\n\n    return mapping, dataset\n    \nmapping, train = fixFeature15(train.copy(deep=True))\nprint(f\"Feature: {feature} has {train[feature].isna().sum()} null values and {len(train[feature].unique())} unique categories\")\n\ncategoricalFixerMappings.update(mapping)\n#Summary: In this feature, the data is well distributed over many categories. It would make a nice plot, but meh. I'll just bucket it\n#Code\na = FloatSlider(min=-3, max=3, step=0.01, value=2.01, description='a', layout=layout)\nb = FloatSlider(min=-3, max=3, step=0.01, value=-2.53, description='b', layout=layout)\nc = FloatSlider(min=-3, max=3, step=0.01, value=1.61, description='c', layout=layout)\nd = FloatSlider(min=-3, max=3, step=0.01, value=-0.33, description='d', layout=layout)\n\ndef f(a, b, c, d):\n    t = 100000\n\n    x = np.empty(t+1)\n    y = np.empty(t+1)\n\n    x[0]=0\n    y[0]=0\n\n    for i in range(t):\n        x[i+1] = np.sin(a*y[i]) + c*np.cos(a*x[i])\n        y[i+1] = np.sin(b*x[i]) + d*np.cos(b*y[i])\n\n\n    fig, ax = plt.subplots(figsize=(7, 7))\n    ax.scatter(x, y, c='b', s=0.01)\n    ax.axis('off');\n    \ninteract(f, a=a, b=b, c=c, d=d);\n#Summary: Pickover attractor\n\n#Code\ndef daddys_atm(train):\n    # Daddy's ATM: First class AND teenager AND alone or with a nanny AND spent more than 40 dollars\n    train['daddys_atm'] = np.where((train['Age'] < 16) &\n                                   (train['SibSp'] < 1) &\n                                   (train['Parch'] < 1) &                                   \n                                   (train['Fare'] > 40),\n                                   1, 0)\n    return train['daddys_atm']\n\ntrain['daddys_atm'] = daddys_atm(train)\n#Summary:", "Daddy's ATM\nFirst class AND teenager AND alone or with a nanny AND spent more than 40 dollars"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef K2M(k):\n    return (k/1.6)\n\nK2M(100)\n#Summary: Python Function to Convert Kilometers to Miles\n#Code\ntqdm.pandas()\ndef las_word_eq_cal(row):\n  que1=row['question1'].split();\n  que2=row['question2'].split();\n  if(len(que1)==0 or len(que2)==0):\n    return 0.0\n  if(que1[len(que1)-1]==que2[len(que2)-1]):\n    return 1.0;\n  else:\n    return 0.0;\ndf['las_word_eq']=df.progress_apply(las_word_eq_cal,axis=1)\n#Summary: \nlast_word_equal= 1 if last word of both questions are equal else 0\n\n#Code\ndef reverse_a_string(string):\n        return string[::-1]\nreverse_a_string(\"\")\n\n#or\n\nprint(\"\".join(reversed(\"Harkirat\")))\n#Summary: Write a Python Program that prints the reversed version of a string.\nThe program must preserve uppercase and lowercase letters.\nIf the string is empty, print it intact.\n#Code\nnested_list = [[\"a\", 1], [\"b\", 2], [\"c\", 3], [\"d\", 4]]\n\ndef dict_from_nested_list(nested_list):\n    my_dict = {}\n    for item in nested_list:\n        my_dict.update({item[0]:item[1]})\n    return my_dict\ndict_from_nested_list([[\"a\", 1], [\"b\", 2], [\"c\", 3], [\"d\", 4]])\n#Summary: Write a Python program that creates a dictionary from the values contained in nested lists.\n\nEach nested list has this format [value1, value2].\n\nvalue1 should be the key in the dictionary and value2 should be its corresponding value.\n\nIf there are no nested lists, print an empty dictionary.\n#Code\ndef calculate_realized_volatility(df, m):\n    \n    # Calculate realized volatility by window\n    df[\"realized_volatility_\" + str(m)] = np.sqrt(df[\"sqrd_log_return_\" + str(m) + \"_sum\"])\n        \n    return df\n#Summary: realized volatility\n\n#Code\ndef who_the_hell_are_you(train):\n    # Who the hell are you?: A person sharing the room with someone that is not from their family and not a nanny or child.\n    train['who_the_hell_are_you'] = np.where((train['Pclass'] > 1) &\n                                             (train['SibSp'] + train ['Parch']) > 1 &\n                                             (train['Fare'] > 10),\n                                             1, 0)\n    return train['who_the_hell_are_you']\n\ntrain['who_the_hell_are_you'] = who_the_hell_are_you(train)\n#Summary:", "Who the hell are you?\nA person sharing the room with someone that is not from their family and not a nanny or child[insert]"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef imputing_embarked(dataframe, embarked_column):\n\n    dataframe[embarked_column] = dataframe[embarked_column].fillna('cherbourg')\n\n    return dataframe\n\ntrain_titanic = imputing_embarked(train_titanic, 'Embarked')\n#Summary: Imputing the \"Embarked\" column in the training data using the values from similar passengers to the missing ones.\n#Code\ndef dropping_train_id(train_data, id_column):\n    \n    train_data = train_data.drop(columns = [id_column])\n\n    return train_data\n\ntrain_titanic = dropping_train_id(train_titanic, 'PassengerId')\n#Summary: Dropping the \"PassengerId\" column from the training data.\n#Code\ndef merging_training_validation(dataframe_training, dataframe_validation, target):\n\n    dataframe_validation['DataClass'] = 'validation'\n\n    y_variable = dataframe_training[target]\n    dataframe_training['DataClass'] = 'train'\n    dataframe_training = dataframe_training.drop(columns = [target])\n\n    dataframe = pd.concat([dataframe_training, dataframe_validation])\n\n    return dataframe, y_variable\n\ntitanic_dataframe, y_variable = merging_training_validation(train_titanic, validation_titanic, 'Survived')\n#Summary: Merging training and validation datasets to perform the encoding of all the categorical columns.\n#Code\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n#Summary: Define the loss function\n#Code\ndef wait_a_sec_hold_on(train):\n    # Wait a sec. hold on!: Young male alone with nannies everywhere (or at least one nanny).\n    train['wait_a_sec_hold_on'] = np.where((train['Age'] < 30) &\n                                           (train['Sex'] == 'male') &\n                                           (train['SibSp'] == 0) &\n                                           (train['Parch'] == 1),\n                                           1, 0)\n    return train['wait_a_sec_hold_on']\n\ntrain['wait_a_sec_hold_on'] = wait_a_sec_hold_on(train)\n#Summary: Wait a sec. hold on!\nYoung male alone with nannies.\n\n#Code\ndef yolo(train):\n    # YOLO: A person who paid a lower fare but with a better room quality than another passenger.\n    train['yolo'] = np.where((train['Fare'] < train['Fare'].mean() - 3 * train['Fare'].std()) &\n                             (train['Pclass'] > 1),\n                             1, 0)\n    return train['yolo']\n\ntrain['yolo'] = yolo(train)\n#Summary:", "YOLO\nA person who paid a lower fare but with a better room quality"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\nx0 = FloatSlider(min=-20, max=20, step=1, value=0, description='x0', layout=layout)\ny0 = FloatSlider(min=-20, max=20, step=1, value=1, description='y0', layout=layout)\n\na = FloatSlider(min=-1, max=1, step=0.001, value=0.008, description='a', layout=layout)\nb = FloatSlider(min=-1, max=1, step=0.001, value=0.05, description='b', layout=layout)\nm = FloatSlider(min=-1, max=1, step=0.001, value=-0.496, description='m', layout=layout)\n\n\ndef f(x0, y0, a, b, m):\n    t = 100000\n\n    x = np.empty(t+1)\n    y = np.empty(t+1)\n\n    x[0]=x0\n    y[0]=y0\n    \n    def G(x, m):\n        return m*x + 2*(1 - m)*x**2/(1 + x**2)\n\n    for i in range(t):        \n        x[i+1] = y[i] + a*(1 - b*y[i]**2)*y[i] + G(x[i], m)\n        y[i+1] = -x[i] + G(x[i+1], m)\n \n\n    fig, ax = plt.subplots(figsize=(7, 7))\n    ax.scatter(x, y, c='b', s=0.05)\n    ax.axis('off');\n    \ninteract(f, x0=x0, y0=y0, a=a, b=b, m=m);\n#Summary: Gumowski-Mira map\n#Code\ndefault_target_names = ['Deceased', 'Alive']\n\ndef cmatrix(\n  y_test,\n  y_pred,\n  # cmap: str = 'YlGnBu',\n  # cmap: str = 'BlorRd',\n  # cmap: str = 'Reds',\n  # cmap: str = 'RdBu',\n  cmap: str = 'Blues',\n  target_names: List[str] = default_target_names,\n  title: str = 'Example',\n) -> np.ndarray:\n  df_cm = confusion_matrix(y_test, y_pred)\n  df_cm = df_cm.astype('float') / df_cm.sum(axis = 1)[:, np.newaxis]\n  # df_cm = df_cm * 100\n\n  # plt.figure(figsize = (10, 7))\n  # plt.figure(figsize = (20, 10))\n  _ = sns.heatmap(\n    df_cm,\n    square=True,\n    annot=True,\n    # annot_kws={\"size\": 13},\n    annot_kws={'fontsize': 14},\n    cmap=cmap,\n    # fmt='g',\n    xticklabels=target_names,\n    yticklabels=target_names,\n    cbar=True,\n    cbar_kws={'orientation': 'horizontal'},\n  ).set(\n    xlabel='Predicted Class',\n    ylabel='Actual Class',\n    title=f'{title} - Confusion Matrix'\n  )\n  plt.show()\n\n  print(classification_report(y_test, y_pred, target_names=target_names))\n\n  return df_cm\n#Summary: It computes the confusion matrix with some default target names, and it also prints the classification report\n#Code\ndef MaxMinL(L):\n  mx =L[0]\n  mn = L[0]\n  for n in L:\n    if n>mx: \n      mx = n\n    if n<mn:\n      mn = n\n  return mx, mn\nA = [3, 12, -27]\nx, y= MaxMinL(A)\nprint (\"Max = \", x, \" and Min = \", y)\n#Summary: Write a function that returns the max number and the min number in a list\n#Code\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = None\n\ndef mean_normalize(dataframe):\n  global scaler\n\n  backup_normalized_columns = dataframe.columns\n  if not scaler:\n    scaler = MinMaxScaler()\n    scaler.fit(dataframe)\n\n  return pd.DataFrame(scaler.transform(dataframe), columns=backup_normalized_columns)\n#Summary: Data Normalization\nAll the values must be between 0 and 1 with the lowest amount of standard deviation possible so that, having all of the values in the same scale, the models can perform at it's best\n#Code\ndef predict(X, y, parameters): \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    AL, caches = forward_propagation(X, parameters)\n    for i in range(0, AL.shape[1]):\n        if AL[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(str(np.mean((p[0,:] == y[0,:]))))\n    return p\n#Summary: **Predection function to use the model and check the results **\n\n#Code\ndef is_rich(train):\n    # Rich: travelling in first class AND alone or with a nanny AND spent more than 40 dollars\n    train['is_rich'] = np.where((train['Pclass'] == 1) &\n                                (train['SibSp'] < 1) &\n                                (train['Parch'] < 1) &\n                                (train['Fare'] > 40),\n                                1, 0)\n    return train['is_rich']\n\ntrain['is_rich'] = is_rich(train)\n#Summary:", "Is Rich\nTravelling first class AND Alone OR with a nanny AND spent more than 40 dollars"], ["You are an expert Python programmer, please describe the functionality of the method:\n#Code\ndef reverse_a_string(string):\n        return string[::-1]\nreverse_a_string(\"\")\n\n#or\n\nprint(\"\".join(reversed(\"Harkirat\")))\n#Summary: Write a Python Program that prints the reversed version of a string.\nThe program must preserve uppercase and lowercase letters.\nIf the string is empty, print it intact.\n#Code\ndef binning_sibsp_column(dataframe_train, dataframe_validation, sibsp_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[sibsp_column] = k_bins_discretizer.fit_transform(dataframe_train[sibsp_column].values.reshape(-1,1))\n    dataframe_validation[sibsp_column] = k_bins_discretizer.transform(dataframe_validation[sibsp_column].values.reshape(-1,1))\n\n    dataframe_train[sibsp_column] = dataframe_train[sibsp_column].astype('int64')\n    dataframe_validation[sibsp_column] = dataframe_validation[sibsp_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_sibsp_column(train_titanic, validation_titanic, 'SibSp')\n#Summary: Transforming the \"SibSp\" column in categorical Bins.\n#Code\ndef predict(X, y, parameters): \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    AL, caches = forward_propagation(X, parameters)\n    for i in range(0, AL.shape[1]):\n        if AL[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(str(np.mean((p[0,:] == y[0,:]))))\n    return p\n#Summary: **Predection function to use the model and check the results **\n#Code\ntqdm.pandas()\ndef fuzz_set_ratio_cal(row):\n  return fuzz.token_set_ratio(row['question1'],row['question2'])\ndf['fuzz_set_ratio']=df.progress_apply(fuzz_set_ratio_cal,axis=1)\n\n#Summary: \nfuzz_set_ratio\n\n#Code\ndef get_model_for_feature_selection(start, end):\n\n    # Parameters\n    params = {\n        'objective': 'reg:squarederror',\n        'n_estimators' : 10,\n        'tree_method': 'gpu_hist',\n        'seed' : SEED\n    }\n\n    # Instantiation\n    model = xgb.XGBRegressor(**params)\n\n    # Fitting the model\n    model.fit(X_train.iloc[:, start:end], y_train)\n    \n    return model\n#Summary: Build model for feature selection\n\n#Code\ndef is_rich(train):\n    # Rich: travelling in first class AND alone or with a nanny AND spent more than 40 dollars\n    train['is_rich'] = np.where((train['Pclass'] == 1) &\n                                (train['SibSp'] < 1) &\n                                (train['Parch'] < 1) &\n                                (train['Fare'] > 40),\n                                1, 0)\n    return train['is_rich']\n\ntrain['is_rich'] = is_rich(train)\n#Summary:", "Is Rich\nTravelling first class AND Alone OR with a nanny AND spent more than 40 dollars"]]