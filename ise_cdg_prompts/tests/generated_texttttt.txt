{"jj": "Start Markdown 1: (In the previous step we also replaced the categorical variables with their integer codes, since some of the models we'll be building in a moment require that.)\n\nNow we can create our independent variables (the `x` variables) and dependent (the `y` variable):\nStart Code 1: def xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\nStart Markdown 2: # \ud83d\ude84 Training Function\nStart Code 2: def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    return epoch_loss\nStart Markdown 3: ##Separate Lungs\nStart Code 3: def seperate_lungs(image):\n    #Creation of the markers as shown above:\n    marker_internal, marker_external, marker_watershed = generate_markers(image)\n    \n    #Creation of the Sobel-Gradient\n    sobel_filtered_dx = ndimage.sobel(image, 1)\n    sobel_filtered_dy = ndimage.sobel(image, 0)\n    sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n    sobel_gradient *= 255.0 / np.max(sobel_gradient)\n    \n    #Watershed algorithm\n    watershed = morphology.watershed(sobel_gradient, marker_watershed)\n    \n    #Reducing the image created by the Watershed algorithm to its outline\n    outline = ndimage.morphological_gradient(watershed, size=(3,3))\n    outline = outline.astype(bool)\n    \n    #Performing Black-Tophat Morphology for reinclusion\n    #Creation of the disk-kernel and increasing its size a bit\n    blackhat_struct = [[0, 0, 1, 1, 1, 0, 0],\n                       [0, 1, 1, 1, 1, 1, 0],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [1, 1, 1, 1, 1, 1, 1],\n                       [0, 1, 1, 1, 1, 1, 0],\n                       [0, 0, 1, 1, 1, 0, 0]]\n    blackhat_struct = ndimage.iterate_structure(blackhat_struct, 8)\n    #Perform the Black-Hat\n    outline += ndimage.black_tophat(outline, structure=blackhat_struct)\n    \n    #Use the internal marker and the Outline that was just created to generate the lungfilter\n    lungfilter = np.bitwise_or(marker_internal, outline)\n    #Close holes in the lungfilter\n    #fill_holes is not used here, since in some slices the heart would be reincluded by accident\n    lungfilter = ndimage.morphology.binary_closing(lungfilter, structure=np.ones((5,5)), iterations=3)\n    \n    #Apply the lungfilter (note the filtered areas being assigned -2000 HU)\n    segmented = np.where(lungfilter == 1, image, -2000*np.ones((512, 512)))\n    \n    return segmented, lungfilter, outline, watershed, sobel_gradient, marker_internal, marker_external, marker_watershed\nStart Markdown 4: ### Data Cleaning\n\n* Adjust the `Crash Severity ID` since it does not line up in a same step scale as the `Crash Severity`\n    * There are values like `201905.0` and `201904.0` which should just be 5 & 4 respectively.\n    * This lines up with the ID captured in column `CRASHSEVERITY`\n    \n    \n* 7 columns have issues with small variations in category names, for instance 'Other' is encoded as '9 - Other' and '99 - Other', or \"M - Male\" and \"Male\".  I will treat those as the same values;\n    * Columns with this issue present:\n        * `GENDER`\n        * `CRASHSEVERITY`\n        * `INJURIES`\n        * `LIGHTCONDITIONSPRIMARY`\n        * `ROADCONDITIONSPRIMARY`\n        * `ROADSURFACE`\n        * `WEATHER`\n        \n        \n* Change the column type of `LOCALREPORTNO` to 'object'\nStart Code 4: # Fix the Crash Severity ID\ndef CrashSeverityIDFixed(x):\n    if x == 201905.0:\n        return 5\n    elif x == 201904.0:\n        return 4\n    elif x == 201903.0:\n        return 3\n    elif x == 201902.0:\n        return 2\n    elif x == 201901.0:\n        return 1\n    else: \n        return x\n    \ndf['CRASHSEVERITYID'] = df['CRASHSEVERITYID'].apply(lambda x: CrashSeverityIDFixed(x))\n\n# Fix Gender\ndf.loc[(df['GENDER'] == 'MALE'), 'GENDER']='M - MALE'\ndf.loc[(df['GENDER'] == 'FEMALE'), 'GENDER']='F - FEMALE'\n\n# Fix CRASHSEVERITY\ndf.loc[(df['CRASHSEVERITY'] == '1 - FATAL'), 'CRASHSEVERITY']='1 - FATAL INJURY'\ndf.loc[(df['CRASHSEVERITY'] == '3 - PROPERTY DAMAGE ONLY (PDO)'), 'CRASHSEVERITY']='5 - PROPERTY DAMAGE ONLY'\n\n# Create a simple loop to fix the other 6 columns\ncols = ['INJURIES', 'LIGHTCONDITIONSPRIMARY', 'ROADCONDITIONSPRIMARY', 'ROADSURFACE', 'WEATHER', 'CRASHSEVERITY']\n\nfor col in cols:\n    df[col] = df[col].str.split(\" - \", 1).str[1]\n    \n# Adjust some column types\ndf = df.astype({\"CRASHSEVERITYID\": object, \"LOCALREPORTNO\": object})\n\nGenerate markdown for the bottom code according to the four samples above\n Code: from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)"}