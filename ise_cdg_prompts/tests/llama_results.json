[["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: nan\nStart Code 1: nan\nStart Markdown 2: # Transforms for test data using albumentations\nStart Code 2: def test_transforms():\n    return Compose([\n            A.Resize(sz, sz),\n            A.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225], \n                        max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\nStart Markdown 3: ### Display Images\nStart Code 3: def dispaly_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()\nStart Markdown 4: To compare different approaches to dealing with missing values, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model.\nStart Code 4: from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\nThen, Generate markdown for the below code according to the pairs.\nCode: class LastN:\n    def __init__(self, n: int, default_val: int=0):\n        self.n = n\n        self.index = 0\n        self.data = [np.NAN] * 10\n        self.default_val = default_val\n\n    def put(self, item):\n        self.data[self.index] = item\n        if self.index + 1 == self.n - 1:\n            self.index = 0\n        else:\n            self.index += 1\n\n    def avg(self):\n        avg = np.nanmean(self.data)\n        return self.default_val if np.isnan(avg) else avg\n\n\ndef init_stats_dict(_df: pd.DataFrame) -> {str: {}}:\n    teams = set(pd.concat([_df['TEAM_AWAY'], _df['TEAM_HOME']]))\n    return {team: {'elo': 1500,\n                   'last_10_games': LastN(10, 0),\n                   'last_10_elo_diff': LastN(10, 0.5),\n                   'win_home': 0,\n                   'win_away': 0,\n                   'total_games': 0,\n                   'last_game_pt_diff': 0,\n                   'last_10_ast': LastN(10, 20),\n                   'last_10_multi_pt_fga': LastN(10, 100),\n                   'last_10_off_rating': LastN(10, 100),\n                   'last_10_def_rating': LastN(10, 100),\n                   'last_10_dist': LastN(10, 20),\n                   'last_10_pie': LastN(10, 0.5),\n                   'last_10_oreb_ratio': LastN(10, 0.5)\n                   }\n            for team in teams}\n", "Useful class and dictionary used for dataset creation."], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: ### IMPORT LIBRARIES\nStart Code 1: import pandas as pd\nimport numpy as np\nimport copy\n\n#Data Visualization libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport folium\nimport calmap\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\n\n#Some styling\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\n#displaying markdown\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))\n\n#Web scraping tools\n#REQUESTS --> to fetch data from website\nimport requests\nimport json\n\n#BEAUTIFULSOUP  -->parse HTML content \nfrom bs4 import BeautifulSoup  \n\n#Showing full path of datasets\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Disable warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nStart Markdown 2: ## Now it's time to make the data to dataset form\n\n* Input should be ticker, date\n* and single function will get the dividends, price, stmt as dataset \n* label will be the price of speicific date#\nStart Code 2: def get_dataset(ticker, date):\n    '''\n    output - df_dividends, price, stmt and label of price\n    '''\n    pass\nticker = 'AAPL'\nstr_targetDate = '2020-01-15'\nD_STMT = '/kaggle/input/150-tickers-of-companies/financial_stmts'\nD_DIVID = '/kaggle/input/150-tickers-of-companies/dividends'\nD_PRICE = '/kaggle/input/150-tickers-of-companies/historical_prices'\nTIME_INVEST = 20\nLABEL_PRICE = 'close'\nclass DATA_of_TICKER:\n    def __init__(self, ticker,\n                 d_stmt = D_STMT, d_divid = D_DIVID, d_price = D_PRICE,\n                 fun_stmt = get_stmts_data, fun_price = get_price_data, fun_divid = get_dividends_date, \n                 time_divid_period = TIME_DIVID_PERIOD, time_divid_interval = TIME_DIVID_INTERVAL,\n                 num_stmts = NUM_STMTS, num_price_sfb = NUM_START_FROM_BEGIN, num_price_efb = NUM_END_FROM_BEGIN, \n                 list_stmts_index = LIST_STMTS_INDEX, list_price_index = LIST_PRICE_INDEX, \n                 time_invest = TIME_INVEST, label_price = LABEL_PRICE\n                ):\n        self.ticker = ticker\n        \n        self.D_STMT = d_stmt, \n        self.D_DIVID = d_divid\n        self.D_PRICE = d_price\n        \n        self.FUN_STMT = fun_stmt\n        self.FUN_PRICE = fun_price\n        self.FUN_DIVID = fun_divid\n        \n        self.LIST_STMTS_INDEX = list_stmts_index\n        self.LIST_PRICE_INDEX = list_price_index\n        \n        self.TIME_DIVID_PERIOD = time_divid_period\n        self.TIME_DIVID_INTERVAL = time_divid_interval\n        \n        self.NUM_STMTS = num_stmts\n        self.NUM_PRICE_SFB = num_price_sfb\n        self.NUM_PRICE_EFB = num_price_efb\n        \n        self.df_stmt = pd.read_csv(os.path.join(d_stmt, f'{ticker}_1.csv'), index_col = 0)\n        self.df_divid = pd.read_csv(os.path.join(d_divid, f'{ticker}_1_divid.csv'), index_col = 0)\n        self.df_price = pd.read_csv(os.path.join(d_price, f'{ticker}_1_price.csv'), index_col = 0)\n        \n        self.TIME_INVEST = time_invest\n        self.LABEL_PRICE = label_price\n    def _get_stmts_data(self, str_targetDate):\n        return self.FUN_STMT(self.df_stmt, str_targetDate, \n                             num_stmts = self.NUM_STMTS, list_stmts = self.LIST_STMTS_INDEX)\n    \n    def _get_price_data(self, str_targetDate):\n        return self.FUN_PRICE(self.df_price, str_targetDate, \n                              num_sfb=self.NUM_PRICE_SFB, num_efb=self.NUM_PRICE_EFB, list_loc = self.LIST_PRICE_INDEX)\n    \n    def _get_dividends_data(self, str_targetDate):\n        return self.FUN_DIVID(self.df_divid, str_targetDate, \n                              time_period = self.TIME_DIVID_PERIOD, time_interval = self.TIME_DIVID_INTERVAL)\n    \n    def _flatten_df_2_series(self, df):\n        list_col = df.columns\n        series_total = pd.Series(dtype = 'float64')\n        for col in list_col:\n            series = df[col]\n            series_total = series_total.append(series.rename(lambda label: f'{col}_{label}'))\n        return series_total\n    \n    def get_dataset(self, str_targetDate):\n        df_window_stmt = self._get_stmts_data(str_targetDate)\n        if df_window_stmt.empty: \n            return (pd.Series, 0)\n        df_window_price = self._get_price_data(str_targetDate)\n        df_window_divid = self._get_dividends_data(str_targetDate)\n        series_total = pd.Series(dtype = 'float64')\n        for df in [df_window_stmt, df_window_price, df_window_divid]:\n            series_total = series_total.append(self._flatten_df_2_series(df))\n        label = self._get_invest_portion(str_targetDate)\n        return (series_total, label)\n    \n    def _get_invest_portion(self, str_targetDate):\n        df_price = self.df_price\n        series_strDate = df_price.columns\n        index_investBegin = np.where(series_strDate == str_targetDate)[0][0]\n        str_investEnd = series_strDate[index_investBegin + self.TIME_INVEST]\n        price_invest_end = df_price[str_investEnd].loc[self.LABEL_PRICE]\n        price_invest_start = df_price[str_targetDate].loc[self.LABEL_PRICE]\n        return price_invest_end/price_invest_start\n        \n    \ndemo_ticker = DATA_of_TICKER(ticker)\ndemo_ticker.get_dataset(str_targetDate)\n\nStart Markdown 3: nan\nStart Code 3: nan\nStart Markdown 4: # The function of the training process \u2699\ufe0f\nStart Code 4: def train_process(model: torch.nn.Module, num_epochs = 50, model_name = 'modelo', model_path  = ''):\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    g_step=0\n    max_correct=0\n    optimizer = optim.Adam(model3.parameters(), lr = learning_rate)\n    lr_sheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = learning_decay)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        train_loss = 0\n        train_corr = 0\n        for batch, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device, dtype=torch.int64)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            train_pred = output.argmax(dim=1, keepdim=True)\n            train_corr += train_pred.eq(target.view_as(train_pred)).sum().item()\n            train_loss += F.nll_loss(output, target, reduction='sum').item()\n            loss.backward()\n            optimizer.step()\n            g_step += 1\n            ema(model, g_step)\n            if batch % 100 == 0:\n                print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch * len(data), len(train_loader.dataset),\n                    100. * batch / len(train_loader), loss.item()))\n        train_loss /= len(train_loader.dataset)\n        train_accuracy = 100 * train_corr / len(train_loader.dataset)\n        \n        ##Test\n        model.eval()\n        ema.assign(model)\n        test_loss = 0\n        correct = 0\n        total_pred = np.zeros(0)\n        total_target = np.zeros(0)\n        with torch.inference_mode():\n            for data, target in valid_loader: \n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, reduction='sum').item()\n                pred = output.argmax(dim=1, keepdim=True)\n                total_pred = np.append(total_pred, pred.cpu().numpy())\n                total_target = np.append(total_target, target.cpu().numpy())\n                correct += pred.eq(target.view_as(pred)).sum().item()\n            \n            if(max_correct < correct):\n                #torch.save(model.state_dict(), '/kaggle/working/model.pt')\n                model_path = Path(model_path)\n                model_path.mkdir(parents = True, exist_ok = True)\n                model_named = f'{model_name}.pth'\n                model_save_path = model_path / model_named\n                if model_save_path.exists():\n                    print(f'Remove {model_save_path}')\n                    os.remove(model_save_path)\n                print(f'Saving model to: {model_save_path}')\n                torch.save(obj = model.state_dict(), f = model_save_path)\n                max_correct = correct\n                print('Best accuracy: %5d'%correct)    \n        ema.resume(model)\n        \n        #output\n        test_loss /= len(valid_loader.dataset)\n        test_accuracy = 100 * correct / len(valid_loader.dataset)\n        best_test_accuracy = 100 * max_correct / len(valid_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) (best: {:.2f}%)\\n'.format(\n            test_loss, correct, len(valid_loader.dataset), test_accuracy, best_test_accuracy))\n        lr_sheduler.step()\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_accuracy)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_accuracy)    \n    return results\n\nThen, Generate markdown for the below code according to the pairs.\nCode: # Natalia\n# Sex, Smoking Status \ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n    \ndisplay_side_by_side(patient_info_df.groupby(['Sex']).count()['Patient'].to_frame(),\n                 patient_info_df.groupby(['SmokingStatus']).count()['Patient'].to_frame(),\n                 patient_info_df.groupby(['Sex','SmokingStatus']).count()['Patient'].to_frame())\n", "### 3.1.3 Categorical Features: Smoking Status and Sex"], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: ## Feature : Days since review\nStart Code 1: def get_days(days_string) -> int:\n    \n    \"\"\" The function extracts interger value from string\n\n    Args:\n        days_string (object): from DataFrame\n\n    Returns:\n        int: number of days\n    \"\"\"\n    days = days_string.split(sep=' ')[0]\n    \n    return int(days)\n\nhotels['days_since_review'] = hotels['days_since_review'].apply(get_days)\nhotels['days_since_review'] = hotels['days_since_review'].astype('int32')\n\nStart Markdown 2: ## Preprocessing\nTurn the HDF5 into a data-frame and a folder full of TIFF files\nStart Code 2: base_h5 = os.path.join('..', 'input', 'all_mias_scans.h5')\ntif_dir = 'tiffs'\nos.makedirs(tif_dir, exist_ok=True)\nwith h5py.File(base_h5, 'r') as f:\n    mammo_df = pd.DataFrame(\n        {k: v.value if len(v.shape)==1 else [sub_v for sub_v in v] \n         for k,v in f.items()}\n    )\nfor k in mammo_df.columns:\n    if isinstance(mammo_df[k].values[0], bytes):\n        mammo_df[k] = mammo_df[k].map(lambda x: x.decode())\n# save data to disk\nfrom skimage.io import imsave\ndef to_path(c_row):\n    out_path = os.path.join(tif_dir, '%s.tif' % c_row['REFNUM'])\n    imsave(out_path, c_row['scan'])\n    return out_path\nmammo_df['scan'] = mammo_df.apply(to_path,1)\nmammo_df.sample(5)\nStart Markdown 3: ## Function for Visualization\n\nSince we will use the visualization often later, let's make both the graph and the MAE functional.\nStart Code 3: def plot_co2(train, test, y_pred, title):\n    mae = mean_absolute_error(test, y_pred)\n    train[\"1985\":].plot(legend=True, label=\"TRAIN\", title=f\"{title}, MAE: {round(mae,2)}\")\n    test.plot(legend=True, label=\"TEST\", figsize=(6, 4))\n    y_pred.plot(legend=True, label=\"PREDICTION\")\n    plt.show()\nStart Markdown 4: \n**Preparing the data for feature importance**\nStart Code 4: import xgboost as xgb\ntrain_y = df[\"y\"].values\ntrain_X = df.drop(['y'], axis=1)\n\ndef xgb_r2_score(preds, final):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'n_trees': 520, \n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.98,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(train_y), # base prediction = mean(target)\n    'silent': 1\n}\n\nfinal = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params), final, num_boost_round=200, feval=xgb_r2_score, maximize=True)\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(model, max_num_features=40, height=0.8, ax=ax, color = 'coral')\nprint(\"Feature Importance by XGBoost\")\nplt.show()\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:40]\n\nplt.subplots(figsize=(10,10))\nplt.title(\"Feature importances by RandomForestRegressor\")\nplt.ylabel(\"Features\")\nplt.barh(range(len(indices)), importances[indices], color=\"green\", align=\"center\")\nplt.yticks(range(len(indices)), feat_names[indices], rotation='horizontal')\nplt.ylim([-1, len(indices)])\nplt.show()\n\nThen, Generate markdown for the below code according to the pairs.\nCode: '''\n- function: show_image\n- parameters: img = Image (image instance of the image file)\n- return: none\n- description: to plot the image using matplotlib\n'''\ndef show_image(img):\n    plt.figure(figsize=(10,10))\n    plt.imshow(img, cmap='Greys')\n    plt.show()\n", "<span style=\"color: #555555\">I hope this clears your doubt but if it doesn't don't worry most people don't and it really doesn't come in between your progress as you will keep hitting at it until one day it will start making sense automatically.<br/>Without any further adieu, let's look at our image!<br/>But first let's create a function to display images as we are going to need it so much!</span>"], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: Plot History\nStart Code 1: import matplotlib.pyplot as plt\ndef plot_history(history):\n    plt.plot(history.history['categorical_accuracy'])\n    plt.plot(history.history['val_categorical_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\nplot_history(history)\nStart Markdown 2: ### Average User Rating\nStart Code 2: import plotly.graph_objs as go\nimport plotly.offline as py\n\n\n\ntemp_df = df['Average User Rating'].value_counts().reset_index()\n\ntrace1 = go.Bar(\n                x = temp_df['index'],\n                y = temp_df['Average User Rating'],\n                marker = dict(color = '#47E0FF',\n                              line=dict(color='#000000', \n                                        width=1)))\nlayout = go.Layout(template= \"plotly_dark\",\n                   title = 'Average User Rating', \n                   xaxis = dict(title = 'Average User Rating'), \n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()\n\ndef pie_plot(count_series, title):\n    labels=count_series.index\n    values=count_series.values\n    trace = go.Pie(labels=labels, \n                   values=values, \n                   title=title, \n                   hoverinfo='percent+value', \n                   textinfo='percent',\n                   textposition='inside',\n                   hole=0.5,\n                   showlegend=True,\n                   marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 20)),\n                               line=dict(color='#ffffff',\n                                         width=2),\n                              )\n                  )\n    return trace\n\npy.iplot([pie_plot(df['Average User Rating'].value_counts(), 'Average User Rating')])\nStart Markdown 3: nan\nStart Code 3: nan\nStart Markdown 4: <div class='subsection_title'> Other crucial terms </div>\n\n### TPR or True positive rate, which is the same as recall.\n\n## TPR = TP/(TP + FN)\n\n### TPR or Recall is also know as **sensitivity**.\n\nStart Code 4: def tpr(y_true, y_pred):\n    '''\n    Function that return TPR \n    :param y_true: list of true values.\n    :param y_pred: list of predicited values.\n    :return tpr value\n    '''\n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    try:\n        tpr = tp/(tp+fn)\n    except:\n        tpr = 0\n        \n    return tpr\n\n\nThen, Generate markdown for the below code according to the pairs.\nCode: import MiniAttention.MiniAttention as ma\nimport math\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    print(embedding_matrix.shape)\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_scaled_dot_self_product_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n\n", "## Encoder Decoder Model Architecture with Scaled Dot Product Attention\n\nThe architecture is as follows:\n\n<img src=\"https://i.imgur.com/pBnshbv.png\">"], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: ## Create Folds\nStart Code 1: def create_folds(data):\n    \n    # We create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1.0\n    \n    # randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Calculate the number of bins using Sturges's law\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"loss\"], bins=num_bins, labels=False\n    )\n    \n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins\n    for fold, (train_, val_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[val_, 'kfold'] = fold\n        \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n    \n    return data\n\ndf = create_folds(train)\ndf.to_csv(\"kfold_train.csv\", index=False)\nStart Markdown 2: ## 8. <span title=\"Spicy - this optional problem is intended to be extra difficult\" style=\"color: coral\">\ud83c\udf36\ufe0f\ud83c\udf36\ufe0f</span>\n\nRead the function definition below. What do you think will be returned by the function call at the bottom of the cell? Will it match the docstring example? Return something else? Or will Python raise an exception? When you're ready, run the code cell to find out.\nStart Code 2: def smallest_stringy_number(s1, s2, s3):\n    \"\"\"Return whichever of the three string arguments represents the smallest number.\n    \n    >>> smallest_stringy_number('1', '2', '3')\n    '1'\n    \"\"\"\n    return min(s1, s2, s3)\n\nsmallest_stringy_number('b', 'b', 'b')\nStart Markdown 3: # Metrics\n\nThe competition metric is F1 score. But, while working with minibatch of size 16...some minibatch may have True_positives = 0 causing recall and precision value to be 0 resulting in undefined F1 score. I will use simple accuracy here for now.\nStart Code 3: def accuracy(y_pred, y_test):\n    \n    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n    correct_pred = (y_pred_tags == y_test).float()\n    acc = correct_pred.sum() / len(correct_pred)\n    acc = round(acc.item(), 5)\n    \n    return acc\nStart Markdown 4: ## Function to clean data\nStart Code 4: pat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?://[A-Za-z0-9./]+'\ncombined_pat = r'|'.join((pat1, pat2))\ndef tweet_cleaner(text):\n    tok = WordPunctTokenizer()\n\n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    stripped = re.sub(combined_pat, '', souped)\n    try:\n        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        clean = stripped\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n    lower_case = letters_only.lower()\n    # During the letters_only process two lines above, it has created unnecessay white spaces,\n    # I will tokenize and join together to remove unneccessary white spaces\n    words = tok.tokenize(lower_case)\n    return (\" \".join(words)).strip()\n\nThen, Generate markdown for the below code according to the pairs.\nCode: sns.set(rc={'figure.figsize':(10.7,8.27)})\n@interact\ndef distribution_colum(column = list(set(itertools.chain(*high_corr_var)))):\n    sns.distplot(train[column],kde=True)\n", "### Let's visualize them"], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: <a id='dof'></a>\n<div style = \"display: fill;\n              border-radius: 10px;\n              background-color: #e3760f;\">\n    <h2 style = \"padding: 20px; \n                 color: White;\n                 text-align: left;\n                 font-family: Lato;\n                 font-weight: bold;\">Drawing Outlier Function\n    </h2>\n</div>\nStart Code 1: def outlier_graph(df):\n    \n    \"\"\"\n    to draw outlier plots\n    \"\"\"\n    \n    sns.boxplot(x=df)\n    plt.show()\nStart Markdown 2: Q4- Get email domain by function\nStart Code 2: def get_domain(mail):\n    ind = mail.index('@')\n    return mail[ind + 1:]\n\nprint('domain of your mail is :',get_domain('amir@gmail.com'))\nStart Markdown 3: ## Helper Functions\n\nDefine a function to plot a given asset for a given date range. Borrowed initial code from kernel: https://www.kaggle.com/bielrv/two-sigma-extensive-eda. Thanks :)\nStart Code 3: # plotAsset plots assetCode1 from date1 to date2\ndef plotAsset(assetCode1, date1, date2):\n    asset_df = mt_df[(mt_df['assetCode'] == assetCode1) \n                      & (mt_df['time'] > date1) \n                      & (mt_df['time'] < date2)]\n\n    x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = asset_df['close'].values\n\n#    plt.figure(figsize=(10,6))\n    plt.figure(figsize=(8,5))\n    plt.title(assetCode1+\": Opening and closing price\")\n    plt.plot(asset_df.time, asset_df.open, label='Open price')\n    plt.plot(asset_df.time, asset_df.close, label='Close price')\n    plt.legend()\n    plt.show()\n\nStart Markdown 4: # Train Loop\nStart Code 4: # Seed for producing results\nseed_torch(seed=45)\n\n# ====================================================\n# loader\n# ====================================================\ngroups = train[\"path\"].map(lambda x: x.split(\"/\")[5])\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X, y, groups)):\n    train_index = train_index\n    valid_index = valid_index\n    print(f\"Fold {i}:\")\n    print(f\"  Train index shape: {train_index.shape}\")\n    print(f\"         group={groups[train_index]}\")\n    print(f\"  Valid index shape:  {valid_index.shape}\")\n    print(f\"         group={groups[valid_index]}\")\n    break\nX_train, X_val, y_train, y_val = (\n    X[train_index],\n    X[valid_index],\n    y[train_index],\n    y[valid_index],\n)\ntrain_dataset = Dataset(X_train, y_train)\nvalid_dataset = Dataset(X_val, y_val)\n\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n    drop_last=True,\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n    drop_last=False,\n)\n\n\n# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(optimizer):\n    if CFG.scheduler == \"ReduceLROnPlateau\":\n        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n    elif CFG.scheduler == \"CosineAnnealingLR\":\n        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n    elif CFG.scheduler == \"CosineAnnealingWarmRestarts\":\n        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.reduce_params)\n    return scheduler\n\n\n# ====================================================\n# model & optimizer\n# ====================================================\nmodel = ASLLinearModel(\n    in_features=3864,\n    first_out_features=1024,\n    num_classes=250,\n    num_blocks=3,\n    drop_rate=0.4,\n)\nmodel.to(device)\n\noptimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler = get_scheduler(optimizer)\n\n# ====================================================\n# loop\n# ====================================================\ncriterion = nn.CrossEntropyLoss()\nbest_score = 0\nfor epoch in range(CFG.epochs):\n    start_time = time.time()\n\n    # train\n    avg_loss = train_fn(\n        train_loader, model, criterion, optimizer, epoch, scheduler, device\n    )\n\n    # eval\n    avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n\n    if isinstance(scheduler, ReduceLROnPlateau):\n        scheduler.step(avg_val_loss)\n    elif isinstance(scheduler, CosineAnnealingLR):\n        scheduler.step()\n    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n        scheduler.step()\n\n    score = get_score(y_val, preds.argmax(1))\n\n    elapsed = time.time() - start_time\n\n    LOGGER.info(\n        f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n    )\n    LOGGER.info(f\"Epoch {epoch+1} - Score: {score:.4f}\")\n    wandb.log(\n        {\n            f\"epoch\": epoch + 1,\n            f\"avg_train_loss\": avg_loss,\n            f\"avg_val_loss\": avg_val_loss,\n            f\"score\": score,\n        }\n    )\n\n    if best_score < score:\n        best_score = score\n        LOGGER.info(f\"Epoch {epoch+1} - Save Best score: {best_score:.4f} Model\")\n        torch.save(\n            model.state_dict(),\n            OUTPUT_DIR + f\"{CFG.model_name}_best_score_version{VERSION}.pth\",\n        )\nLOGGER.info(f\"Our CV score is {best_score}\")\n\nThen, Generate markdown for the below code according to the pairs.\nCode: #https://github.com/huggingface/transformers/issues/18476\nfrom transformers.tf_utils import shape_list\n\ndef take_along_axis(x, indices, gather_axis):\n    # Only a valid port of np.take_along_axis when the gather axis is -1\n\n    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239\n    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):\n        # [B, S, P] -> [B, S, P, D]\n        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)\n\n        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)\n        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]\n        gathered = tf.einsum(\"ijkl,ijl->ijk\", one_hot_indices, x)\n\n    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls\n    else:\n        gathered = tf.gather(x, indices, batch_dims=2)\n\n    return gathered\n\n\ntransformers.models.deberta_v2.modeling_tf_deberta_v2.take_along_axis = take_along_axis\n\nclass TFDebertaV2StableDropout(tf.keras.layers.Layer):\n    \"\"\"\n    Optimized dropout module for stabilizing the training\n    Args:\n        drop_prob (float): the dropout probabilities\n    \"\"\"\n\n    def __init__(self, drop_prob, **kwargs):\n        super().__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    @tf.custom_gradient\n    def xdropout(self, inputs):\n        \"\"\"\n        Applies dropout to the inputs, as vanilla dropout, but also scales the remaining elements up by 1/drop_prob.\n        \"\"\"\n        mask = tf.cast(\n            1\n            - tf.compat.v1.distributions.Bernoulli(probs=1.0 - self.drop_prob).sample(sample_shape=shape_list(inputs)),\n            tf.bool,\n        )\n        scale = tf.convert_to_tensor(1.0 / (1 - self.drop_prob), dtype=tf.float32)\n        if self.drop_prob > 0:\n            inputs = tf.where(mask, 0.0, inputs) * scale\n\n        def grad(upstream):\n            if self.drop_prob > 0:\n                return tf.where(mask, 0.0, upstream) * scale\n            else:\n                return upstream\n\n        return inputs, grad\n\n    def call(self, inputs: tf.Tensor, training: tf.Tensor = False):\n        if training:\n            return self.xdropout(inputs)\n        return inputs\n\ntransformers.models.deberta_v2.modeling_tf_deberta_v2.TFDebertaV2StableDropout = TFDebertaV2StableDropout\n", "Here are some methods we need to modify to run DeBERTa V3 on TPU. More information refers to [this issue](https://github.com/huggingface/transformers/issues/18476). "], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: Since the samples have different durations, we need to reshape these two-dimensional arrays in order to feed correctly the convolutional model. For this reason, I apply a function to have the files to the same shape of 30x150, which corresponds to about 4.5 seconds of audio. This also means that files that are shorter will have an additional silent part to reach this threshold (zeros padding), while longer files will be cut off.\nStart Code 1: # Define function to resize the 2D arrays\ndef resize_array(array):\n    new_matrix = np.zeros((30,150))   # Initialize the new matrix shape with an array 30X150 of zeros\n    for i in range(30):               # Iterate rows\n        for j in range(150):          # Iterate columns\n            try:                                 # the mfccs of a sample will replace the matrix of zeros, then cutting the array up to 150\n                new_matrix[i][j] = array[i][j]\n            except IndexError:                   # if mfccs of a sample is shorter than 150, then keep looping to extend lenght to 150 with 0s\n                pass\n    return new_matrix\n\n# Create a variable to store the new resized mfccs and apply function for all the extracted mfccs\nresized_mfccs = []\n\nfor mfcc in mfccs:\n    resized_mfccs.append(resize_array(mfcc))\nStart Markdown 2: ## Resizing Images\n\nNow we need a function to get these images in a shape and format where we can feed it to our AI model, Keras doesn't allow us to feed a bunch of images in different sizes and shapes to our ConvNet.\nStart Code 2: def resize_to_fit(self, image, width, height):\n    \"\"\"\n    A helper function to resize an image to fit within a given size\n\n    :param image: image to resize\n    :param width: desired width in pixels\n    :param height: desired height in pixels\n    :return: the resized image\n    \"\"\"\n\n    # grab the dimensions of the image, then initialize\n    # the padding values\n    (h, w) = image.shape[:2]\n\n    # if the width is greater than the height then resize along\n    # the width\n    if w > h:\n        image = imutils.resize(image, width=width)\n\n    # otherwise, the height is greater than the width so resize\n    # along the height\n    else:\n        image = imutils.resize(image, height=height)\n\n    # determine the padding values for the width and height to\n    # obtain the target dimensions\n    padW = int((width - image.shape[1]) / 2.0)\n    padH = int((height - image.shape[0]) / 2.0)\n\n    # pad the image then apply one more resizing to handle any\n    # rounding issues\n    image = cv2.copyMakeBorder(image, padH, padH, padW, padW,\n        cv2.BORDER_REPLICATE)\n    image = cv2.resize(image, (width, height))\n\n    # return the pre-processed image\n    return image\nStart Markdown 3: Read data from input file and split it to train/dev set\nStart Code 3: def read_dataset(filename):\n    df = pd.read_csv(filename, low_memory=False)\n    msk = np.random.rand(len(df)) < 0.85\n    dev = df[~msk]\n    train = df[msk]\n\n    X_train = process_X(df)\n    Y_train = df['Survived'].values\n\n    return X_train, Y_train\n\nStart Markdown 4: # Q4-first\nStart Code 4: def domain(name):\n  print(name.split('@')[1])\n\ndomain('user@gmail.com')\n\nThen, Generate markdown for the below code according to the pairs.\nCode: def Ticket_Prefix(s):\n    s=s.split()[0]\n    if s.isdigit():\n        return 'NoClue'\n    else:\n        return s\n\ndf['TicketPrefix'] = df['Ticket'].apply(lambda x: Ticket_Prefix(x))\n\ndf.head()\n", "The ticket prefix may determine the status or cabin on board and hence will be included"], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: 3. Searching by track_id\nStart Code 1: def get_song_info(id):\n    artist = music_info.loc[music_info.track_id == id].artist\n    name = music_info.loc[music_info.track_id == id].name\n    info = pd.merge(artist, name, right_index=True, left_index=True)\n    return info\n\ntrack_id = 'TRLNZBD128F935E4D8'\nget_song_info(track_id)\nStart Markdown 2: This function will plot projections onto a vector.\n\nStart Code 2: def plot_proj(A,v,y,name=None):\n\n    plt.scatter(A[:,0],A[:,1],label='data',c=y,cmap='viridis')\n    \n    #plt.plot(np.linspace(A[:,0].min(),A[:,0].max()),np.linspace(A[:,1].min(),A[:,1].max())*(v[1]/v[0]),color='black',linestyle='--',linewidth=1.5,label=name)   \n    plt.plot(np.linspace(-1,1),np.linspace(-1,1)*(v[1]/v[0]),color='black',linestyle='--',linewidth=1.5,label=name)  \n    # Run through all the data\n\n    for i in range(len(A[:,0])-1):\n        #data point \n        w=A[i,:]\n\n        # projection\n        cv = (np.dot( A[i,:],v))/np.dot(v,np.transpose(v))*v\n\n        # line between data point and projection\n        plt.plot([A[i,0],cv[0]],[A[i,1],cv[1]],'r--',linewidth=1.5)\n    plt.plot([A[-1,0],cv[0]],[A[-1,1],cv[1]],'r--',linewidth=1.5,label='projections' )\n    plt.legend()\n    plt.show()\nStart Markdown 3: #### Fitting All Models One at the Time\nStart Code 3: def model_select(classifier):\n    cv_result = []\n    cv_means = []\n    # Cross validate model with Kfold stratified cross val\n    kfold = StratifiedKFold(n_splits=5)\n    cv_result.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    cv_means.append(np.mean(cv_result))\n    return cv_means\n# Fitting all the models \nmodel_type = [KNeighborsClassifier(),GaussianNB(),RandomForestClassifier(),\n              AdaBoostClassifier(),GradientBoostingClassifier(),DecisionTreeClassifier(),ExtraTreesClassifier()]\nmodel_score = [model_select(i) for i in model_type]\nStart Markdown 4: ### Now which one do we go for? For this example, I am gonna go with 4 topics because the target lables in this dataset was classified into 4. Otherwise, its better to go with the one having the highest coherence score.\n\n### The purpose of LDA is also to compute how much of the document was generated by which topic. Lets look at that.\nStart Code 4: def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n   \n    sent_topics_df = pd.DataFrame()\n\n   \n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # -- dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    \n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n    \n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df)\n\n\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n\ndf_dominant_topic.head(5)\n\nThen, Generate markdown for the below code according to the pairs.\nCode: def apply_model(clf, X_train, y_train):\n    \n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_score = clf.predict_proba(X_test)[:, 1]\n    target_names = ['class 0', 'class 1']\n    \n    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_, normalize='true')\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=clf.classes_)\n    \n    print('ROC-AUC score is %0.4f' % roc_auc_score(y_test, y_score))\n    print('Classification report:\\n', classification_report(y_test, y_pred, target_names=target_names))\n    print('Confusion matrix:')\n    disp.plot()\n", "First of all, I will create a function, with the help of which it will be fast to apply any model and get desired metrics, so that I would not need to repeat the same code over and over again."], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: ### Learning Rate  <a id=\"3.7\"></a>\nStart Code 1: LR_START = 0.00001\nLR_MAX = 0.0001 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 15\nLR_SUSTAIN_EPOCHS = 3\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = [\n  #tf.keras.callbacks.EarlyStopping(patience=7),\n  tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n]\nStart Markdown 2: ### 2.2.1 | Univariate Outliers\n\n* Grubbs Test : detect whether there are outliers in our dataset or not\n* Z-score method : 3 sigma\nStart Code 2: # In our project, we choose Z-score\noutliers = []\n\ndef z_score_detector(df):\n    mean_ = df.mean()\n    std = df.std()\n    \n    for i, value in enumerate(df):\n        z = (value - mean_) / std\n        if abs(z) > 3:\n            outliers.append(i)\n\nStart Markdown 3: ### Building Vocabulary\nIn this we assign aunique integer to every possible word in the data set\nStart Code 3: # Build the vocabulary\n# Unit Test Note - There is no test set here only train/val\ndef get_vocab(train_data):\n\n    # Include special tokens \n    # started with pad, end of line and unk tokens\n    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n\n    # Note that we build vocab using training data\n    for tweet in train_data:\n        processtweet= process_tweet(tweet)\n        for word in processtweet:\n            if word not in Vocab: \n                Vocab[word] = len(Vocab)\n    \n    return Vocab\n\nVocab = get_vocab(df['text'])\n\nprint(\"Total words in vocab are\",len(Vocab))\nStart Markdown 4: the above picture show to formulars, that tells the same thing. The thing it tells is how we go from input(x) though the model and to mse to make the target prediction. \nStart Code 4: def mse_grad(inp, targ): \n    # gradient of loss function with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n    #the mse is just the input squared subratet with the target and the derevitive of that is 2 times input minus target \n    #we have to store the gradient somewhere since for the chain rule, we have to multiply all the these things together (see above picture)\n    #so we can store it in the dot g (.g) of the previous layer. So the input of the mse is the same as the output of the previous layer\n\nThen, Generate markdown for the below code according to the pairs.\nCode: def reshape_batch(images, labels):\n    images = tf.reshape(images, shape=[BATCH_SIZE, IMG_TARGET_SIZE, IMG_TARGET_SIZE, N_CHANNELS])\n    labels = tf.reshape(labels, shape=[BATCH_SIZE, N_LABELS])\n    \n    random_idxs = tf.random.shuffle(tf.range(BATCH_SIZE))\n    images = tf.gather(images, random_idxs)\n    labels = tf.gather(labels, random_idxs)\n    \n    return images, labels\n", "Reshaping the batch from [x,y,...] to [x*y,...]. This greatly improves the dataset speed in comparison with unbatching and batching."], ["For now, Just read these template Markdown and Code pairs.\nStart Markdown 1: ### Delete Memory\n\nWe create a function to delete objects to free up GPU.\nStart Code 1: import torch\nimport gc\n\n\ndef free_gpu_memory(\n    *args,\n) -> None:\n    \"\"\"Delete all variables from the GPU. Clear cache.\n    Args:\n        model ([type], optional): [description]. Defaults to None.\n        optimizer (torch.optim, optional): [description]. Defaults to None.\n        scheduler (torch.optim.lr_scheduler, optional): [description]. Defaults to None.\n    \"\"\"\n\n#     if args is not None:\n#         # Delete all other variables\n#         for arg in args:\n#             for var_name, unique_var_id in globals().items():\n#                 if id(arg) == id(unique_var_id):\n#                     del globals()[var_name]\n\n    gc.collect()\n    torch.cuda.empty_cache()\nStart Markdown 2: **Initialize**\nStart Code 2: def initalize_weight_and_bias(dimension):\n    w=np.full((dimension,1),0.01) \n    b=0.0\n    return w,b\nStart Markdown 3: (In the previous step we also replaced the categorical variables with their integer codes, since some of the models we'll be building in a moment require that.)\n\nNow we can create our independent variables (the `x` variables) and dependent (the `y` variable):\nStart Code 3: def xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\nStart Markdown 4: # \ud83d\ude84 Training Function\nStart Code 4: def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    return epoch_loss\n\nThen, Generate markdown for the below code according to the pairs.\nCode: def trj_speed(trj_name):\n    grouped = trj_data.loc[trj_data['trj_label'] == trj_name].groupby('driver')\n    plt.figure(figsize=(15,10))\n    for ind,g in grouped:\n        g.reset_index(inplace=True)\n        plt.plot(g[\"dist\"], g[\"speed_norm\"], '.-', label = g.loc[0,\"driver\"])\n\n    plt.grid()\n    plt.ylabel(\"Norm. speed [avg={:.2f} km/h]\".format(g[\"speed\"].mean()))\n    plt.xlabel(\"Distance traveled [km]\")\n    plt.legend()\n\ntrj_speed('trj_1')\ntrj_speed('trj_2')\n", "## Plots of the normalized speed vs linear position for the selected subsets"]]